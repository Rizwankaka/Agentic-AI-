{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsW+x4mbCtgrx8J9pknoCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rizwankaka/Agentic-AI-/blob/main/SmolAgents/smolagents_agentic_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## change the runtype T4"
      ],
      "metadata": {
        "id": "dopfc9L0uYmc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-VinFEvaspqQ"
      },
      "outputs": [],
      "source": [
        "%pip install -qU smolagents litellm langchain langchain-community sentence-transformers datasets chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "NSpQsggetB6z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import CodeAgent, HfApiModel\n",
        "\n",
        "model = HfApiModel()\n",
        "\n",
        "agent = CodeAgent(tools=[],model=model)\n",
        "\n",
        "agent.run('What is 24*365?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "vFILnY1-td5b",
        "outputId": "73736a77-ad06-4745-c97f-1a90f311f9f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is 24*365?\u001b[0m                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is 24*365?</span>                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m24\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m*\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m365\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">result </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">24</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">*</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">365</span><span style=\"background-color: #272822\">                                                                                              </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(result)</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;212;183;2mOut - Final answer: 8760\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: 8760</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 0: Duration 0.08 seconds| Input tokens: 2,016 | Output tokens: 58]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 0.08 seconds| Input tokens: 2,016 | Output tokens: 58]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8760"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla RAG has limitations, most importantly these two:\n",
        "1. It performs only one retrieval step: if the results are bad, the generation in turn will be bad.\n",
        "\n",
        "2. The user query will often be a question and the document containing the true answer will be in affirmative voice, so its similarity score will be downgraded compared to other source documents in the interrogative form, leading to a risk of missing the relevant information.\n",
        "\n",
        "This Agent will:\n",
        "\n",
        "- ✅ Formulate the query itself\n",
        "- ✅ Critique to re-retrieve if needed."
      ],
      "metadata": {
        "id": "epxQPCFAtqO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## indexing data into chroma"
      ],
      "metadata": {
        "id": "mnpXJ63It4Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "#from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
        "\n",
        "source_docs = [\n",
        "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
        "    for doc in knowledge_base\n",
        "]\n",
        "\n",
        "### Creating Chunks using RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "new_docs = text_splitter.split_documents(documents=source_docs)\n",
        "\n",
        "###  BGE Embddings\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "### Populate Vector DB\n",
        "\n",
        "db = Chroma.from_documents(new_docs, embeddings)"
      ],
      "metadata": {
        "id": "ICI4-kAmt-6j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "retriever.invoke('forward pass in transformer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCBhfPM2vFAj",
        "outputId": "f5cb8b94-cb0f-4a05-f2f4-b37a2c03c369"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'transformers', 'start_index': 7863}, page_content='4.  [ ] Created script that successfully runs forward pass using\\n    original repository and checkpoint\\n\\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\\n\\n6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original checkpoint\\n\\n8.  [ ] Finished model tests in Transformers\\n\\n9.  [ ] Successfully added Tokenizer in Transformers'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7510}, page_content='4.  [ ] Created script that successfully runs forward pass using\\n    original repository and checkpoint\\n\\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\\n\\n6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original checkpoint\\n\\n8.  [ ] Finished model tests in Transformers\\n\\n9.  [ ] Successfully added Tokenizer in Transformers'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7863}, page_content='4.  [ ] Created script that successfully runs forward pass using\\n    original repository and checkpoint\\n\\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\\n\\n6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original checkpoint\\n\\n8.  [ ] Finished model tests in Transformers\\n\\n9.  [ ] Successfully added Tokenizer in Transformers'),\n",
              " Document(metadata={'source': 'transformers', 'start_index': 7510}, page_content='4.  [ ] Created script that successfully runs forward pass using\\n    original repository and checkpoint\\n\\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\\n\\n6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original checkpoint\\n\\n8.  [ ] Finished model tests in Transformers\\n\\n9.  [ ] Successfully added Tokenizer in Transformers')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of Retriever Tool"
      ],
      "metadata": {
        "id": "KU6eg-Q0vH7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        assert isinstance(query, str), \"Your search query must be a string\"\n",
        "\n",
        "        docs = self.retriever.invoke(\n",
        "            query,\n",
        "        )\n",
        "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
        "                for i, doc in enumerate(docs)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "retriever_tool = RetrieverTool()"
      ],
      "metadata": {
        "id": "8ZKxHZcavOHx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## agent initialization"
      ],
      "metadata": {
        "id": "xr3mv-8svWD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import HfApiModel, CodeAgent\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[retriever_tool], model=HfApiModel(), max_steps=4,\n",
        ")"
      ],
      "metadata": {
        "id": "4nig_1IlvalJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_output = agent.run(\"For a transformers model training, which is slower, the forward or the backward pass?\")\n",
        "\n",
        "print(\"Final output:\")\n",
        "print(agent_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LQ8AA5UMvwSM",
        "outputId": "9a3ef15a-e7fa-4a63-afde-bbd9bff9f264"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mFor a transformers model training, which is slower, the forward or the backward pass?\u001b[0m                           \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">For a transformers model training, which is slower, the forward or the backward pass?</span>                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mforward_pass_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mforward pass in transformers model training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mbackward pass in transformers model training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                           \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mForward Pass Info:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mforward_pass_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mBackward Pass Info:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">forward_pass_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"forward pass in transformers model training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                             </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">backward_pass_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"backward pass in transformers model training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                           </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Forward Pass Info:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, forward_pass_info)</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Backward Pass Info:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, backward_pass_info)</span><span style=\"background-color: #272822\">                                                               </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "Forward Pass Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "It is very likely that the 🤗 Transformers implementation and the original model implementation don't give the \n",
              "exact\n",
              "same output the very first time or that the forward pass throws an error. Don't be disappointed - it's expected! \n",
              "First,\n",
              "you should make sure that the forward pass doesn't throw any errors. It often happens that the wrong dimensions are\n",
              "used leading to a *Dimensionality mismatch* error or that the wrong data type object is used, *e.g.* `torch.long`\n",
              "\n",
              "===== Document 1 =====\n",
              "It is very likely that the 🤗 Transformers implementation and the original model implementation don't give the \n",
              "exact\n",
              "same output the very first time or that the forward pass throws an error. Don't be disappointed - it's expected! \n",
              "First,\n",
              "you should make sure that the forward pass doesn't throw any errors. It often happens that the wrong dimensions are\n",
              "used leading to a *Dimensionality mismatch* error or that the wrong data type object is used, *e.g.* `torch.long`\n",
              "\n",
              "===== Document 2 =====\n",
              "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, \n",
              "and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides \n",
              "access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it \n",
              "on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique\n",
              "\n",
              "===== Document 3 =====\n",
              "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, \n",
              "and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides \n",
              "access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it \n",
              "on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique\n",
              "Backward Pass Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "Having managed to correctly load the pretrained weights into the 🤗\n",
              "Transformers implementation, you should now make sure that the forward\n",
              "pass is correctly implemented. In [Get familiar with the original\n",
              "repository](#34-run-a-pretrained-checkpoint-using-the-original-repository),\n",
              "you have already created a script that runs a forward pass of the model\n",
              "using the original repository. Now you should write an analogous script\n",
              "using the 🤗 Transformers implementation instead of the original one. It\n",
              "\n",
              "===== Document 1 =====\n",
              "Having managed to correctly load the pretrained weights into the 🤗\n",
              "Transformers implementation, you should now make sure that the forward\n",
              "pass is correctly implemented. In [Get familiar with the original\n",
              "repository](#34-run-a-pretrained-checkpoint-using-the-original-repository),\n",
              "you have already created a script that runs a forward pass of the model\n",
              "using the original repository. Now you should write an analogous script\n",
              "using the 🤗 Transformers implementation instead of the original one. It\n",
              "\n",
              "===== Document 2 =====\n",
              "- Avoid storing the intermediate results of each layer by using reversible transformer layers to obtain them during\n",
              "the backward pass (subtracting the residuals from the input of the next layer gives them back) or recomputing them \n",
              "for results inside a given layer (less efficient than storing them but saves memory).\n",
              "- Compute the feedforward operations by chunks and not on the whole batch.\n",
              "\n",
              "===== Document 3 =====\n",
              "- Avoid storing the intermediate results of each layer by using reversible transformer layers to obtain them during\n",
              "the backward pass (subtracting the residuals from the input of the next layer gives them back) or recomputing them \n",
              "for results inside a given layer (less efficient than storing them but saves memory).\n",
              "- Compute the feedforward operations by chunks and not on the whole batch.\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "Forward Pass Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "It is very likely that the 🤗 Transformers implementation and the original model implementation don't give the \n",
              "exact\n",
              "same output the very first time or that the forward pass throws an error. Don't be disappointed - it's expected! \n",
              "First,\n",
              "you should make sure that the forward pass doesn't throw any errors. It often happens that the wrong dimensions are\n",
              "used leading to a *Dimensionality mismatch* error or that the wrong data type object is used, *e.g.* `torch.long`\n",
              "\n",
              "===== Document 1 =====\n",
              "It is very likely that the 🤗 Transformers implementation and the original model implementation don't give the \n",
              "exact\n",
              "same output the very first time or that the forward pass throws an error. Don't be disappointed - it's expected! \n",
              "First,\n",
              "you should make sure that the forward pass doesn't throw any errors. It often happens that the wrong dimensions are\n",
              "used leading to a *Dimensionality mismatch* error or that the wrong data type object is used, *e.g.* `torch.long`\n",
              "\n",
              "===== Document 2 =====\n",
              "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, \n",
              "and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides \n",
              "access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it \n",
              "on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique\n",
              "\n",
              "===== Document 3 =====\n",
              "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, \n",
              "and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides \n",
              "access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it \n",
              "on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique\n",
              "Backward Pass Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "Having managed to correctly load the pretrained weights into the 🤗\n",
              "Transformers implementation, you should now make sure that the forward\n",
              "pass is correctly implemented. In [Get familiar with the original\n",
              "repository](#34-run-a-pretrained-checkpoint-using-the-original-repository),\n",
              "you have already created a script that runs a forward pass of the model\n",
              "using the original repository. Now you should write an analogous script\n",
              "using the 🤗 Transformers implementation instead of the original one. It\n",
              "\n",
              "===== Document 1 =====\n",
              "Having managed to correctly load the pretrained weights into the 🤗\n",
              "Transformers implementation, you should now make sure that the forward\n",
              "pass is correctly implemented. In [Get familiar with the original\n",
              "repository](#34-run-a-pretrained-checkpoint-using-the-original-repository),\n",
              "you have already created a script that runs a forward pass of the model\n",
              "using the original repository. Now you should write an analogous script\n",
              "using the 🤗 Transformers implementation instead of the original one. It\n",
              "\n",
              "===== Document 2 =====\n",
              "- Avoid storing the intermediate results of each layer by using reversible transformer layers to obtain them during\n",
              "the backward pass (subtracting the residuals from the input of the next layer gives them back) or recomputing them \n",
              "for results inside a given layer (less efficient than storing them but saves memory).\n",
              "- Compute the feedforward operations by chunks and not on the whole batch.\n",
              "\n",
              "===== Document 3 =====\n",
              "- Avoid storing the intermediate results of each layer by using reversible transformer layers to obtain them during\n",
              "the backward pass (subtracting the residuals from the input of the next layer gives them back) or recomputing them \n",
              "for results inside a given layer (less efficient than storing them but saves memory).\n",
              "- Compute the feedforward operations by chunks and not on the whole batch.\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 0: Duration 2.58 seconds| Input tokens: 2,099 | Output tokens: 99]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 2.58 seconds| Input tokens: 2,099 | Output tokens: 99]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mforward_pass_speed_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mspeed of forward pass in transformers model training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m              \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_speed_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mspeed of backward pass in transformers model training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mForward Pass Speed Info:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mforward_pass_speed_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                     \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mBackward Pass Speed Info:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_speed_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">forward_pass_speed_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"speed of forward pass in transformers model training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">              </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">backward_pass_speed_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"speed of backward pass in transformers model training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">            </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Forward Pass Speed Info:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, forward_pass_speed_info)</span><span style=\"background-color: #272822\">                                                     </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Backward Pass Speed Info:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, backward_pass_speed_info)</span><span style=\"background-color: #272822\">                                                   </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "Forward Pass Speed Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, \n",
              "and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides \n",
              "access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it \n",
              "on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique\n",
              "\n",
              "===== Document 1 =====\n",
              "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, \n",
              "and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides \n",
              "access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it \n",
              "on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique\n",
              "\n",
              "===== Document 2 =====\n",
              "### Expected speedups\n",
              "\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "<div style=\"text-align: center\">\n",
              "<img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\">\n",
              "</div>\n",
              "\n",
              "===== Document 3 =====\n",
              "### Expected speedups\n",
              "\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "<div style=\"text-align: center\">\n",
              "<img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\">\n",
              "</div>\n",
              "Backward Pass Speed Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-350m` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "<div style=\"text-align: center\">\n",
              "<img src=\"https://user-images.githubusercontent.com/49240599/281101682-d1144e90-0dbc-46f4-8fc8-c6206cb793c9.png\">\n",
              "</div>\n",
              "\n",
              "\n",
              "\n",
              "## OPTConfig\n",
              "\n",
              "[[autodoc]] OPTConfig\n",
              "\n",
              "<frameworkcontent>\n",
              "<pt>\n",
              "\n",
              "## OPTModel\n",
              "\n",
              "===== Document 1 =====\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-350m` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "<div style=\"text-align: center\">\n",
              "<img src=\"https://user-images.githubusercontent.com/49240599/281101682-d1144e90-0dbc-46f4-8fc8-c6206cb793c9.png\">\n",
              "</div>\n",
              "\n",
              "\n",
              "\n",
              "## OPTConfig\n",
              "\n",
              "[[autodoc]] OPTConfig\n",
              "\n",
              "<frameworkcontent>\n",
              "<pt>\n",
              "\n",
              "## OPTModel\n",
              "\n",
              "===== Document 2 =====\n",
              "### Expected speedups\n",
              "\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "<div style=\"text-align: center\">\n",
              "<img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\">\n",
              "</div>\n",
              "\n",
              "===== Document 3 =====\n",
              "### Expected speedups\n",
              "\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "<div style=\"text-align: center\">\n",
              "<img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\">\n",
              "</div>\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "Forward Pass Speed Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, \n",
              "and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides \n",
              "access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it \n",
              "on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique\n",
              "\n",
              "===== Document 1 =====\n",
              "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, \n",
              "and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides \n",
              "access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it \n",
              "on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique\n",
              "\n",
              "===== Document 2 =====\n",
              "### Expected speedups\n",
              "\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "&lt;div style=\"text-align: center\"&gt;\n",
              "&lt;img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\"&gt;\n",
              "&lt;/div&gt;\n",
              "\n",
              "===== Document 3 =====\n",
              "### Expected speedups\n",
              "\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "&lt;div style=\"text-align: center\"&gt;\n",
              "&lt;img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\"&gt;\n",
              "&lt;/div&gt;\n",
              "Backward Pass Speed Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-350m` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "&lt;div style=\"text-align: center\"&gt;\n",
              "&lt;img src=\"https://user-images.githubusercontent.com/49240599/281101682-d1144e90-0dbc-46f4-8fc8-c6206cb793c9.png\"&gt;\n",
              "&lt;/div&gt;\n",
              "\n",
              "\n",
              "\n",
              "## OPTConfig\n",
              "\n",
              "[[autodoc]] OPTConfig\n",
              "\n",
              "&lt;frameworkcontent&gt;\n",
              "&lt;pt&gt;\n",
              "\n",
              "## OPTModel\n",
              "\n",
              "===== Document 1 =====\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-350m` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "&lt;div style=\"text-align: center\"&gt;\n",
              "&lt;img src=\"https://user-images.githubusercontent.com/49240599/281101682-d1144e90-0dbc-46f4-8fc8-c6206cb793c9.png\"&gt;\n",
              "&lt;/div&gt;\n",
              "\n",
              "\n",
              "\n",
              "## OPTConfig\n",
              "\n",
              "[[autodoc]] OPTConfig\n",
              "\n",
              "&lt;frameworkcontent&gt;\n",
              "&lt;pt&gt;\n",
              "\n",
              "## OPTModel\n",
              "\n",
              "===== Document 2 =====\n",
              "### Expected speedups\n",
              "\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "&lt;div style=\"text-align: center\"&gt;\n",
              "&lt;img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\"&gt;\n",
              "&lt;/div&gt;\n",
              "\n",
              "===== Document 3 =====\n",
              "### Expected speedups\n",
              "\n",
              "Below is an expected speedup diagram that compares pure inference time between the native implementation in \n",
              "transformers using `facebook/opt-2.7b` checkpoint and the Flash Attention 2 version of the model using two \n",
              "different sequence lengths.\n",
              "\n",
              "&lt;div style=\"text-align: center\"&gt;\n",
              "&lt;img src=\"https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png\"&gt;\n",
              "&lt;/div&gt;\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 3.39 seconds| Input tokens: 5,201 | Output tokens: 227]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.39 seconds| Input tokens: 5,201 | Output tokens: 227]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mforward_pass_complexity_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcomputational complexity of forward pass in transformers model \u001b[0m  \n",
              "  \u001b[38;2;230;219;116;48;2;39;40;34mtraining\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_complexity_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcomputational complexity of backward pass in transformers \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
              "  \u001b[38;2;230;219;116;48;2;39;40;34mmodel training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mForward Pass Complexity Info:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mforward_pass_complexity_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mBackward Pass Complexity Info:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_complexity_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">forward_pass_complexity_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"computational complexity of forward pass in transformers model </span>  \n",
              "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                     </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">backward_pass_complexity_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"computational complexity of backward pass in transformers </span><span style=\"background-color: #272822\">    </span>  \n",
              "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">model training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                               </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Forward Pass Complexity Info:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, forward_pass_complexity_info)</span><span style=\"background-color: #272822\">                                           </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Backward Pass Complexity Info:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, backward_pass_complexity_info)</span><span style=\"background-color: #272822\">                                         </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "Forward Pass Complexity Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent \n",
              "efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the \n",
              "self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified \n",
              "sparsity patterns, low-rank basis expansions and combinations thereof\n",
              "\n",
              "===== Document 1 =====\n",
              "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent \n",
              "efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the \n",
              "self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified \n",
              "sparsity patterns, low-rank basis expansions and combinations thereof\n",
              "\n",
              "===== Document 2 =====\n",
              "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models\n",
              "can\n",
              "be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\n",
              "Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\n",
              "complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible \n",
              "residual\n",
              "\n",
              "===== Document 3 =====\n",
              "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models\n",
              "can\n",
              "be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\n",
              "Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\n",
              "complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible \n",
              "residual\n",
              "Backward Pass Complexity Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models\n",
              "can\n",
              "be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\n",
              "Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\n",
              "complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible \n",
              "residual\n",
              "\n",
              "===== Document 1 =====\n",
              "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models\n",
              "can\n",
              "be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\n",
              "Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\n",
              "complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible \n",
              "residual\n",
              "\n",
              "===== Document 2 =====\n",
              "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent \n",
              "efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the \n",
              "self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified \n",
              "sparsity patterns, low-rank basis expansions and combinations thereof\n",
              "\n",
              "===== Document 3 =====\n",
              "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent \n",
              "efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the \n",
              "self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified \n",
              "sparsity patterns, low-rank basis expansions and combinations thereof\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "Forward Pass Complexity Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent \n",
              "efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the \n",
              "self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified \n",
              "sparsity patterns, low-rank basis expansions and combinations thereof\n",
              "\n",
              "===== Document 1 =====\n",
              "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent \n",
              "efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the \n",
              "self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified \n",
              "sparsity patterns, low-rank basis expansions and combinations thereof\n",
              "\n",
              "===== Document 2 =====\n",
              "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models\n",
              "can\n",
              "be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\n",
              "Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\n",
              "complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible \n",
              "residual\n",
              "\n",
              "===== Document 3 =====\n",
              "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models\n",
              "can\n",
              "be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\n",
              "Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\n",
              "complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible \n",
              "residual\n",
              "Backward Pass Complexity Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models\n",
              "can\n",
              "be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\n",
              "Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\n",
              "complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible \n",
              "residual\n",
              "\n",
              "===== Document 1 =====\n",
              "*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models\n",
              "can\n",
              "be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\n",
              "Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\n",
              "complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible \n",
              "residual\n",
              "\n",
              "===== Document 2 =====\n",
              "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent \n",
              "efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the \n",
              "self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified \n",
              "sparsity patterns, low-rank basis expansions and combinations thereof\n",
              "\n",
              "===== Document 3 =====\n",
              "*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent \n",
              "efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the \n",
              "self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified \n",
              "sparsity patterns, low-rank basis expansions and combinations thereof\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 4.24 seconds| Input tokens: 9,571 | Output tokens: 390]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 4.24 seconds| Input tokens: 9,571 | Output tokens: 390]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mforward_pass_time_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtime taken by forward pass in transformers model training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m          \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_time_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtime taken by backward pass in transformers model training\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m        \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mForward Pass Time Info:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mforward_pass_time_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mBackward Pass Time Info:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_time_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                     \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">forward_pass_time_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"time taken by forward pass in transformers model training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">          </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">backward_pass_time_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"time taken by backward pass in transformers model training\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">        </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Forward Pass Time Info:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, forward_pass_time_info)</span><span style=\"background-color: #272822\">                                                       </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Backward Pass Time Info:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, backward_pass_time_info)</span><span style=\"background-color: #272822\">                                                     </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "Forward Pass Time Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "===== Document 1 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "===== Document 2 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "===== Document 3 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "Backward Pass Time Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "It suggests a tweak in the traditional Transformer attention to make it linear. This way, the model can be used as \n",
              "recurrent network: passing inputs for timestamp 0 and timestamp 1 together is the same as passing inputs at \n",
              "timestamp 0, then inputs at timestamp 1 along with the state of timestamp 0 (see example below).\n",
              "\n",
              "This can be more efficient than a regular Transformer and can deal with sentence of any length (even if the model \n",
              "uses a fixed context length for training).\n",
              "\n",
              "===== Document 1 =====\n",
              "It suggests a tweak in the traditional Transformer attention to make it linear. This way, the model can be used as \n",
              "recurrent network: passing inputs for timestamp 0 and timestamp 1 together is the same as passing inputs at \n",
              "timestamp 0, then inputs at timestamp 1 along with the state of timestamp 0 (see example below).\n",
              "\n",
              "This can be more efficient than a regular Transformer and can deal with sentence of any length (even if the model \n",
              "uses a fixed context length for training).\n",
              "\n",
              "===== Document 2 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "===== Document 3 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "Forward Pass Time Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "===== Document 1 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "===== Document 2 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "===== Document 3 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "Backward Pass Time Info: \n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "It suggests a tweak in the traditional Transformer attention to make it linear. This way, the model can be used as \n",
              "recurrent network: passing inputs for timestamp 0 and timestamp 1 together is the same as passing inputs at \n",
              "timestamp 0, then inputs at timestamp 1 along with the state of timestamp 0 (see example below).\n",
              "\n",
              "This can be more efficient than a regular Transformer and can deal with sentence of any length (even if the model \n",
              "uses a fixed context length for training).\n",
              "\n",
              "===== Document 1 =====\n",
              "It suggests a tweak in the traditional Transformer attention to make it linear. This way, the model can be used as \n",
              "recurrent network: passing inputs for timestamp 0 and timestamp 1 together is the same as passing inputs at \n",
              "timestamp 0, then inputs at timestamp 1 along with the state of timestamp 0 (see example below).\n",
              "\n",
              "This can be more efficient than a regular Transformer and can deal with sentence of any length (even if the model \n",
              "uses a fixed context length for training).\n",
              "\n",
              "===== Document 2 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "===== Document 3 =====\n",
              "than 10 seconds. In case only very large checkpoints are available,\n",
              "    it might make more sense to create a dummy model in the new\n",
              "    environment with randomly initialized weights and save those weights\n",
              "    for comparison with the 🤗 Transformers version of your model\n",
              "-   Make sure you are using the easiest way of calling a forward pass in\n",
              "    the original repository. Ideally, you want to find the function in\n",
              "    the original repository that **only** calls a single forward pass,\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 3: Duration 4.43 seconds| Input tokens: 14,954 | Output tokens: 561]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.43 seconds| Input tokens: 14,954 | Output tokens: 561]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mReached max steps.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Reached max steps.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Final answer: In the context of training transformers, the backward pass is generally slower than the forward pass.\n",
              "This is because the backward pass involves computing the gradients of the loss with respect to each parameter, \n",
              "which requires additional operations such as backpropagation through the entire network. These operations can be \n",
              "computationally intensive, especially for large models with many parameters and layers.\n",
              "\n",
              "The forward pass, on the other hand, simply involves passing the input data through the model to obtain \n",
              "predictions, which is typically less computationally intensive.\n",
              "\n",
              "While the exact time difference can vary depending on the model architecture, the size of the input data, and the \n",
              "hardware used, it is a common observation in deep learning that the backward pass is slower than the forward pass.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final answer: In the context of training transformers, the backward pass is generally slower than the forward pass.\n",
              "This is because the backward pass involves computing the gradients of the loss with respect to each parameter, \n",
              "which requires additional operations such as backpropagation through the entire network. These operations can be \n",
              "computationally intensive, especially for large models with many parameters and layers.\n",
              "\n",
              "The forward pass, on the other hand, simply involves passing the input data through the model to obtain \n",
              "predictions, which is typically less computationally intensive.\n",
              "\n",
              "While the exact time difference can vary depending on the model architecture, the size of the input data, and the \n",
              "hardware used, it is a common observation in deep learning that the backward pass is slower than the forward pass.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 4: Duration 0.00 seconds| Input tokens: 19,510 | Output tokens: 705]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 0.00 seconds| Input tokens: 19,510 | Output tokens: 705]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output:\n",
            "In the context of training transformers, the backward pass is generally slower than the forward pass. This is because the backward pass involves computing the gradients of the loss with respect to each parameter, which requires additional operations such as backpropagation through the entire network. These operations can be computationally intensive, especially for large models with many parameters and layers.\n",
            "\n",
            "The forward pass, on the other hand, simply involves passing the input data through the model to obtain predictions, which is typically less computationally intensive.\n",
            "\n",
            "While the exact time difference can vary depending on the model architecture, the size of the input data, and the hardware used, it is a common observation in deep learning that the backward pass is slower than the forward pass.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"For a transformers model training, What is the role of scaled dot product?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h4BgVAJmwSYI",
        "outputId": "9814e82b-d639-4889-8c99-ed24caf6e6f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mFor a transformers model training, What is the role of scaled dot product?\u001b[0m                                      \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">For a transformers model training, What is the role of scaled dot product?</span>                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mscaled_dot_product_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mscaled dot product in transformers\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mscaled_dot_product_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">scaled_dot_product_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"scaled dot product in transformers\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(scaled_dot_product_info)</span><span style=\"background-color: #272822\">                                                                                 </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "## BetterTransformer\n",
              "\n",
              "<Tip warning={true}>\n",
              "\n",
              "Some BetterTransformer features are being upstreamed to Transformers with default support for native \n",
              "`torch.nn.scaled_dot_product_attention`. BetterTransformer still has a wider coverage than the Transformers SDPA \n",
              "integration, but you can expect more and more architectures to natively support SDPA in Transformers.\n",
              "\n",
              "</Tip>\n",
              "\n",
              "<Tip>\n",
              "\n",
              "===== Document 1 =====\n",
              "## BetterTransformer\n",
              "\n",
              "<Tip warning={true}>\n",
              "\n",
              "Some BetterTransformer features are being upstreamed to Transformers with default support for native \n",
              "`torch.nn.scaled_dot_product_attention`. BetterTransformer still has a wider coverage than the Transformers SDPA \n",
              "integration, but you can expect more and more architectures to natively support SDPA in Transformers.\n",
              "\n",
              "</Tip>\n",
              "\n",
              "<Tip>\n",
              "\n",
              "===== Document 2 =====\n",
              "The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear activation \n",
              "`GeLU`.\n",
              "The dot dot-product part of it, following the Megatron's paper notation, can be written as `Y = GeLU(XA)`, where \n",
              "`X` is \n",
              "an input vector, `Y` is the output vector, and `A` is the weight matrix.\n",
              "\n",
              "If we look at the computation in matrix form, you can see how the matrix multiplication can be split between \n",
              "multiple GPUs:\n",
              "\n",
              "===== Document 3 =====\n",
              "The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear activation \n",
              "`GeLU`.\n",
              "The dot dot-product part of it, following the Megatron's paper notation, can be written as `Y = GeLU(XA)`, where \n",
              "`X` is \n",
              "an input vector, `Y` is the output vector, and `A` is the weight matrix.\n",
              "\n",
              "If we look at the computation in matrix form, you can see how the matrix multiplication can be split between \n",
              "multiple GPUs:\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "## BetterTransformer\n",
              "\n",
              "&lt;Tip warning={true}&gt;\n",
              "\n",
              "Some BetterTransformer features are being upstreamed to Transformers with default support for native \n",
              "`torch.nn.scaled_dot_product_attention`. BetterTransformer still has a wider coverage than the Transformers SDPA \n",
              "integration, but you can expect more and more architectures to natively support SDPA in Transformers.\n",
              "\n",
              "&lt;/Tip&gt;\n",
              "\n",
              "&lt;Tip&gt;\n",
              "\n",
              "===== Document 1 =====\n",
              "## BetterTransformer\n",
              "\n",
              "&lt;Tip warning={true}&gt;\n",
              "\n",
              "Some BetterTransformer features are being upstreamed to Transformers with default support for native \n",
              "`torch.nn.scaled_dot_product_attention`. BetterTransformer still has a wider coverage than the Transformers SDPA \n",
              "integration, but you can expect more and more architectures to natively support SDPA in Transformers.\n",
              "\n",
              "&lt;/Tip&gt;\n",
              "\n",
              "&lt;Tip&gt;\n",
              "\n",
              "===== Document 2 =====\n",
              "The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear activation \n",
              "`GeLU`.\n",
              "The dot dot-product part of it, following the Megatron's paper notation, can be written as `Y = GeLU(XA)`, where \n",
              "`X` is \n",
              "an input vector, `Y` is the output vector, and `A` is the weight matrix.\n",
              "\n",
              "If we look at the computation in matrix form, you can see how the matrix multiplication can be split between \n",
              "multiple GPUs:\n",
              "\n",
              "===== Document 3 =====\n",
              "The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear activation \n",
              "`GeLU`.\n",
              "The dot dot-product part of it, following the Megatron's paper notation, can be written as `Y = GeLU(XA)`, where \n",
              "`X` is \n",
              "an input vector, `Y` is the output vector, and `A` is the weight matrix.\n",
              "\n",
              "If we look at the computation in matrix form, you can see how the matrix multiplication can be split between \n",
              "multiple GPUs:\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 0: Duration 1.68 seconds| Input tokens: 2,097 | Output tokens: 68]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 1.68 seconds| Input tokens: 2,097 | Output tokens: 68]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mscaled_dot_product_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mscaled dot product attention in transformers\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                      \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mscaled_dot_product_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">scaled_dot_product_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"scaled dot product attention in transformers\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                      </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(scaled_dot_product_info)</span><span style=\"background-color: #272822\">                                                                                 </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "## PyTorch scaled dot product attention\n",
              "\n",
              "PyTorch's \n",
              "[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.\n",
              "scaled_dot_product_attention.html) (SDPA) can also call FlashAttention and memory-efficient attention kernels under\n",
              "the hood. SDPA support is currently being added natively in Transformers and is used by default for `torch>=2.1.1` \n",
              "when an implementation is available.\n",
              "\n",
              "===== Document 1 =====\n",
              "## PyTorch scaled dot product attention\n",
              "\n",
              "PyTorch's \n",
              "[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.\n",
              "scaled_dot_product_attention.html) (SDPA) can also call FlashAttention and memory-efficient attention kernels under\n",
              "the hood. SDPA support is currently being added natively in Transformers and is used by default for `torch>=2.1.1` \n",
              "when an implementation is available.\n",
              "\n",
              "===== Document 2 =====\n",
              "*Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token \n",
              "mixer module contributes most to their competence. However, recent works show the attention-based module in \n",
              "transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this \n",
              "observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer \n",
              "module, is more essential to the model's performance\n",
              "\n",
              "===== Document 3 =====\n",
              "*Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token \n",
              "mixer module contributes most to their competence. However, recent works show the attention-based module in \n",
              "transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this \n",
              "observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer \n",
              "module, is more essential to the model's performance\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "## PyTorch scaled dot product attention\n",
              "\n",
              "PyTorch's \n",
              "[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.\n",
              "scaled_dot_product_attention.html) (SDPA) can also call FlashAttention and memory-efficient attention kernels under\n",
              "the hood. SDPA support is currently being added natively in Transformers and is used by default for `torch&gt;=2.1.1` \n",
              "when an implementation is available.\n",
              "\n",
              "===== Document 1 =====\n",
              "## PyTorch scaled dot product attention\n",
              "\n",
              "PyTorch's \n",
              "[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.\n",
              "scaled_dot_product_attention.html) (SDPA) can also call FlashAttention and memory-efficient attention kernels under\n",
              "the hood. SDPA support is currently being added natively in Transformers and is used by default for `torch&gt;=2.1.1` \n",
              "when an implementation is available.\n",
              "\n",
              "===== Document 2 =====\n",
              "*Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token \n",
              "mixer module contributes most to their competence. However, recent works show the attention-based module in \n",
              "transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this \n",
              "observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer \n",
              "module, is more essential to the model's performance\n",
              "\n",
              "===== Document 3 =====\n",
              "*Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token \n",
              "mixer module contributes most to their competence. However, recent works show the attention-based module in \n",
              "transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this \n",
              "observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer \n",
              "module, is more essential to the model's performance\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 1.89 seconds| Input tokens: 4,729 | Output tokens: 144]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 1.89 seconds| Input tokens: 4,729 | Output tokens: 144]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mscaled_dot_product_role_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mrole of scaled dot product in self-attention mechanism\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m       \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mscaled_dot_product_role_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">scaled_dot_product_role_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"role of scaled dot product in self-attention mechanism\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">       </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(scaled_dot_product_role_info)</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic \n",
              "complexity,\n",
              "local attention weakens two of the most desirable properties of self attention: long range inter-dependency \n",
              "modeling,\n",
              "and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible \n",
              "and\n",
              "efficient extension to NA that can capture more global context and expand receptive fields exponentially at no\n",
              "\n",
              "===== Document 1 =====\n",
              "or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic \n",
              "complexity,\n",
              "local attention weakens two of the most desirable properties of self attention: long range inter-dependency \n",
              "modeling,\n",
              "and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible \n",
              "and\n",
              "efficient extension to NA that can capture more global context and expand receptive fields exponentially at no\n",
              "\n",
              "===== Document 2 =====\n",
              "*We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for \n",
              "vision.\n",
              "NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore \n",
              "enjoys a\n",
              "linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's\n",
              "receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike\n",
              "\n",
              "===== Document 3 =====\n",
              "*We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for \n",
              "vision.\n",
              "NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore \n",
              "enjoys a\n",
              "linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's\n",
              "receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic \n",
              "complexity,\n",
              "local attention weakens two of the most desirable properties of self attention: long range inter-dependency \n",
              "modeling,\n",
              "and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible \n",
              "and\n",
              "efficient extension to NA that can capture more global context and expand receptive fields exponentially at no\n",
              "\n",
              "===== Document 1 =====\n",
              "or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic \n",
              "complexity,\n",
              "local attention weakens two of the most desirable properties of self attention: long range inter-dependency \n",
              "modeling,\n",
              "and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible \n",
              "and\n",
              "efficient extension to NA that can capture more global context and expand receptive fields exponentially at no\n",
              "\n",
              "===== Document 2 =====\n",
              "*We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for \n",
              "vision.\n",
              "NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore \n",
              "enjoys a\n",
              "linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's\n",
              "receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike\n",
              "\n",
              "===== Document 3 =====\n",
              "*We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for \n",
              "vision.\n",
              "NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore \n",
              "enjoys a\n",
              "linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's\n",
              "receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 2.21 seconds| Input tokens: 7,913 | Output tokens: 234]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.21 seconds| Input tokens: 7,913 | Output tokens: 234]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting this code:\u001b[0m ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mscaled_dot_product_role_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mrole of scaled dot product in self-attention mechanism of \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
              "  \u001b[38;2;230;219;116;48;2;39;40;34mtransformers\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mscaled_dot_product_role_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing this code:</span> ────────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">scaled_dot_product_role_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"role of scaled dot product in self-attention mechanism of </span><span style=\"background-color: #272822\">     </span>  \n",
              "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">transformers\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                 </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(scaled_dot_product_role_info)</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key \n",
              "component\n",
              "that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence \n",
              "or\n",
              "dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on \n",
              "the\n",
              "input sequence length has limited its application to longer sequences -- a topic being actively studied in the\n",
              "\n",
              "===== Document 1 =====\n",
              "*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key \n",
              "component\n",
              "that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence \n",
              "or\n",
              "dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on \n",
              "the\n",
              "input sequence length has limited its application to longer sequences -- a topic being actively studied in the\n",
              "\n",
              "===== Document 2 =====\n",
              "The main problem with the self-attention mechanism of the Transformer is that the time and memory requirements \n",
              "scale\n",
              "quadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of\n",
              "512\n",
              "tokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a \n",
              "set\n",
              "of latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements \n",
              "don't\n",
              "\n",
              "===== Document 3 =====\n",
              "The main problem with the self-attention mechanism of the Transformer is that the time and memory requirements \n",
              "scale\n",
              "quadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of\n",
              "512\n",
              "tokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a \n",
              "set\n",
              "of latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements \n",
              "don't\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key \n",
              "component\n",
              "that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence \n",
              "or\n",
              "dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on \n",
              "the\n",
              "input sequence length has limited its application to longer sequences -- a topic being actively studied in the\n",
              "\n",
              "===== Document 1 =====\n",
              "*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key \n",
              "component\n",
              "that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence \n",
              "or\n",
              "dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on \n",
              "the\n",
              "input sequence length has limited its application to longer sequences -- a topic being actively studied in the\n",
              "\n",
              "===== Document 2 =====\n",
              "The main problem with the self-attention mechanism of the Transformer is that the time and memory requirements \n",
              "scale\n",
              "quadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of\n",
              "512\n",
              "tokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a \n",
              "set\n",
              "of latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements \n",
              "don't\n",
              "\n",
              "===== Document 3 =====\n",
              "The main problem with the self-attention mechanism of the Transformer is that the time and memory requirements \n",
              "scale\n",
              "quadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of\n",
              "512\n",
              "tokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a \n",
              "set\n",
              "of latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements \n",
              "don't\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 3: Duration 2.37 seconds| Input tokens: 11,651 | Output tokens: 333]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 2.37 seconds| Input tokens: 11,651 | Output tokens: 333]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mReached max steps.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Reached max steps.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Final answer: The scaled dot product is a crucial component of the self-attention mechanism used in Transformer \n",
              "models. It plays a key role in how the model computes the attention weights between different tokens in a sequence.\n",
              "Here's a breakdown of its role:\n",
              "\n",
              "1. **Computation of Attention Scores**: The scaled dot product helps in computing the raw attention scores between \n",
              "each pair of tokens. Given two vectors, \\(Q\\) (query) and \\(K\\) (key), the dot product \\(Q \\cdot K^T\\) is computed.\n",
              "This operation captures the similarity between the query and key vectors.\n",
              "\n",
              "2. **Scaling**: The dot product scores are scaled by dividing by the square root of the dimensionality of the key \n",
              "vector (\\(\\sqrt{d_k}\\)). This scaling step helps in preventing the dot product scores from growing too large, which\n",
              "could lead to very small gradients during training (a problem known as the vanishing gradient problem).\n",
              "\n",
              "3. **Softmax Function**: The scaled dot product scores are then passed through a softmax function to convert them \n",
              "into attention weights. These weights indicate the importance of each token in the sequence when making predictions\n",
              "about a particular token.\n",
              "\n",
              "4. **Weighted Sum**: The attention weights are used to compute a weighted sum of the value vectors (\\(V\\)), \n",
              "resulting in the final context vector for each token. This context vector is a combination of all the tokens in the\n",
              "sequence, weighted by their importance as determined by the attention mechanism.\n",
              "\n",
              "The scaled dot product attention mechanism is mathematically defined as:\n",
              "\n",
              "\\[\n",
              "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
              "\\]\n",
              "\n",
              "Where:\n",
              "- \\(Q\\) is the query matrix,\n",
              "- \\(K\\) is the key matrix,\n",
              "- \\(V\\) is the value matrix,\n",
              "- \\(d_k\\) is the dimensionality of the key vectors.\n",
              "\n",
              "This mechanism allows the Transformer model to dynamically weigh the importance of different tokens when making \n",
              "predictions, enabling it to capture complex dependencies in the input data.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final answer: The scaled dot product is a crucial component of the self-attention mechanism used in Transformer \n",
              "models. It plays a key role in how the model computes the attention weights between different tokens in a sequence.\n",
              "Here's a breakdown of its role:\n",
              "\n",
              "1. **Computation of Attention Scores**: The scaled dot product helps in computing the raw attention scores between \n",
              "each pair of tokens. Given two vectors, \\(Q\\) (query) and \\(K\\) (key), the dot product \\(Q \\cdot K^T\\) is computed.\n",
              "This operation captures the similarity between the query and key vectors.\n",
              "\n",
              "2. **Scaling**: The dot product scores are scaled by dividing by the square root of the dimensionality of the key \n",
              "vector (\\(\\sqrt{d_k}\\)). This scaling step helps in preventing the dot product scores from growing too large, which\n",
              "could lead to very small gradients during training (a problem known as the vanishing gradient problem).\n",
              "\n",
              "3. **Softmax Function**: The scaled dot product scores are then passed through a softmax function to convert them \n",
              "into attention weights. These weights indicate the importance of each token in the sequence when making predictions\n",
              "about a particular token.\n",
              "\n",
              "4. **Weighted Sum**: The attention weights are used to compute a weighted sum of the value vectors (\\(V\\)), \n",
              "resulting in the final context vector for each token. This context vector is a combination of all the tokens in the\n",
              "sequence, weighted by their importance as determined by the attention mechanism.\n",
              "\n",
              "The scaled dot product attention mechanism is mathematically defined as:\n",
              "\n",
              "\\[\n",
              "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
              "\\]\n",
              "\n",
              "Where:\n",
              "- \\(Q\\) is the query matrix,\n",
              "- \\(K\\) is the key matrix,\n",
              "- \\(V\\) is the value matrix,\n",
              "- \\(d_k\\) is the dimensionality of the key vectors.\n",
              "\n",
              "This mechanism allows the Transformer model to dynamically weigh the importance of different tokens when making \n",
              "predictions, enabling it to capture complex dependencies in the input data.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 4: Duration 0.00 seconds| Input tokens: 13,990 | Output tokens: 749]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 0.00 seconds| Input tokens: 13,990 | Output tokens: 749]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The scaled dot product is a crucial component of the self-attention mechanism used in Transformer models. It plays a key role in how the model computes the attention weights between different tokens in a sequence. Here's a breakdown of its role:\\n\\n1. **Computation of Attention Scores**: The scaled dot product helps in computing the raw attention scores between each pair of tokens. Given two vectors, \\\\(Q\\\\) (query) and \\\\(K\\\\) (key), the dot product \\\\(Q \\\\cdot K^T\\\\) is computed. This operation captures the similarity between the query and key vectors.\\n\\n2. **Scaling**: The dot product scores are scaled by dividing by the square root of the dimensionality of the key vector (\\\\(\\\\sqrt{d_k}\\\\)). This scaling step helps in preventing the dot product scores from growing too large, which could lead to very small gradients during training (a problem known as the vanishing gradient problem).\\n\\n3. **Softmax Function**: The scaled dot product scores are then passed through a softmax function to convert them into attention weights. These weights indicate the importance of each token in the sequence when making predictions about a particular token.\\n\\n4. **Weighted Sum**: The attention weights are used to compute a weighted sum of the value vectors (\\\\(V\\\\)), resulting in the final context vector for each token. This context vector is a combination of all the tokens in the sequence, weighted by their importance as determined by the attention mechanism.\\n\\nThe scaled dot product attention mechanism is mathematically defined as:\\n\\n\\\\[\\n\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V\\n\\\\]\\n\\nWhere:\\n- \\\\(Q\\\\) is the query matrix,\\n- \\\\(K\\\\) is the key matrix,\\n- \\\\(V\\\\) is the value matrix,\\n- \\\\(d_k\\\\) is the dimensionality of the key vectors.\\n\\nThis mechanism allows the Transformer model to dynamically weigh the importance of different tokens when making predictions, enabling it to capture complex dependencies in the input data.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}